{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQW6QOkaeJaE"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-1rEQEoFW8aK"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# pytorch imports\n",
        "\n",
        "## dataset and data loader dependencies\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T\n",
        "# image processing dependencies\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import utils as vutils\n",
        "# model dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.tensorboard import SummaryWriter # logger\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiLeGSkC4SlX"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.image.inception import InceptionScore\n",
        "from torchmetrics.functional import (psnr, ssim, accuracy) # pip install torchmetrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72c4pERsjQz"
      },
      "source": [
        "# configuration Options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_BADZEkJslW7"
      },
      "outputs": [],
      "source": [
        "device           = torch.device(\"cuda\")  \n",
        "test_name         = \"aerial_to_street\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zhYhcbZdsmwf"
      },
      "outputs": [],
      "source": [
        "    \n",
        "    # dataset configurations.\n",
        "    result_dataset_dir = f\"results_{test_name}\"  \n",
        "    real_train_data = '/common/home/vs734/task3/task3_as/arial_to_street/train'\n",
        "\n",
        "    valid_data = '/common/home/vs734/task3/task3_as/arial_to_street/val'\n",
        "\n",
        "    IMG_HEIGHT = 256\n",
        "    IMG_WIDTH = 256\n",
        "                  \n",
        "    batch_size            = 48   # Training batch size.\n",
        "    image_size = 256\n",
        "    n_gpus=1\n",
        "    global_batch_size=n_gpus*batch_size\n",
        "    \n",
        "        \n",
        "    # Train params.\n",
        "    epochs                = 200  \n",
        "    lr = 0.0002  \n",
        "\n",
        "\n",
        "    # Training log.\n",
        "    writer                = SummaryWriter(os.path.join(f\"{result_dataset_dir}/stats\",  \"logs\", test_name))\n",
        "    \n",
        "    # Additional variables and create directories\n",
        "    models_dir =  os.path.join(f\"{result_dataset_dir}/models\", test_name)\n",
        "    samples_dir = os.path.join(f\"{result_dataset_dir}/samples\", test_name)\n",
        "    results_dir = os.path.join(f\"{result_dataset_dir}/results\", test_name)\n",
        "    \n",
        "    \n",
        "    os.makedirs(result_dataset_dir, exist_ok=True)\n",
        "    os.makedirs(samples_dir, exist_ok=True)\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    os.makedirs(\"results\",exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0k1atcbXR9U"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1khZWjYVXOZL"
      },
      "outputs": [],
      "source": [
        "# method to get individual images from a batch\n",
        "def load_image(image):\n",
        "    image = np.array(image)\n",
        "    width = image.shape[1]\n",
        "    half_val = width // 2\n",
        "\n",
        "    real_image = image[:, :half_val, :] # left section\n",
        "    conditonal_image = image[:, half_val:, :] # right section\n",
        "\n",
        "    real_image = real_image.astype(np.float32) # convert to float\n",
        "    conditonal_image = conditonal_image.astype(np.float32) # convert to float\n",
        "\n",
        "    return real_image, conditonal_image # return real and conditional images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkp2Rxq5Xax3"
      },
      "source": [
        "## image transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "giq8HKUcXajj"
      },
      "outputs": [],
      "source": [
        "# method to crop images to 256x256\n",
        "# we use a random crop instead of center crop\n",
        "def random_crop(image, dim):\n",
        "    height, width, _ = dim\n",
        "    x, y = np.random.uniform(low=0,high=int(height-256)), np.random.uniform(low=0,high=int(width-256))  \n",
        "    return image[:, int(x):int(x)+256, int(y):int(y)+256]\n",
        "\n",
        "# random jittering and horizantal flipping for data augmentation\n",
        "# this is a function that takes in an image and returns the same image with some random transformations\n",
        "# cv2 is used to perform the transformations\n",
        "def random_jittering_mirroring(input_image, target_image, height=256, width=256):\n",
        "    #resizing to 256x256\n",
        "    input_image = cv2.resize(input_image, (height, width) ,interpolation=cv2.INTER_NEAREST)\n",
        "    target_image = cv2.resize(target_image, (height, width),\n",
        "                               interpolation=cv2.INTER_NEAREST)\n",
        "    \n",
        "    #cropping (random jittering) to 256x256\n",
        "    stacked_image = np.stack([input_image, target_image], axis=0)\n",
        "    cropped_image = random_crop(stacked_image, dim=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    \n",
        "    input_image, target_image = cropped_image[0], cropped_image[1]\n",
        "  \n",
        "    if torch.rand(()) > 0.5: # 50% chance of flipping\n",
        "        # random mirroring\n",
        "        input_image = np.fliplr(input_image)\n",
        "        target_image = np.fliplr(target_image)\n",
        "    return input_image, target_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P5UKRfIdXhPa"
      },
      "outputs": [],
      "source": [
        "# normalize the image to be between -1 and 1\n",
        "# reduces learning time and improves convergence\n",
        "def normalize(inp, tar):\n",
        "    input_image = (inp / 127.5) - 1\n",
        "    target_image = (tar / 127.5) - 1\n",
        "    return input_image, target_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SbmmbhVtXjBv"
      },
      "outputs": [],
      "source": [
        "# Transform function\n",
        "class Transformations(object):\n",
        "    def __call__(self, image):\n",
        "        inp, tar = load_image(image)\n",
        "        inp, tar = random_jittering_mirroring(inp, tar)\n",
        "        inp, tar = normalize(inp, tar)\n",
        "        image_a = torch.from_numpy(inp.copy().transpose((2,0,1)))\n",
        "        image_b = torch.from_numpy(tar.copy().transpose((2,0,1)))\n",
        "        return image_a, image_b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoaders"
      ],
      "metadata": {
        "id": "QvyLS2rX5Hs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train data loader"
      ],
      "metadata": {
        "id": "ILCWwZ_15Jd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpIT5niUXpqe"
      },
      "outputs": [],
      "source": [
        "train_ds = ImageFolder(real_train_data, transform=transforms.Compose([Train()]))\n",
        "train_dl = DataLoader(train_ds, global_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test data loader\n"
      ],
      "metadata": {
        "id": "ODnvcmKi5Lsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-nxUevvXwLq"
      },
      "outputs": [],
      "source": [
        "valid_ds = ImageFolder(valid_data, transform=transforms.Compose([Train()]))\n",
        "valid_dl = DataLoader(valid_ds, global_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HAE5WfSiwNl"
      },
      "source": [
        "# Model weight intialsations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zUn34DINikxI"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on generator and discriminator\n",
        "\n",
        "def init_weights(net, init_type='normal', scaling=0.02):\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv')) != -1:\n",
        "            torch.nn.init.normal_(m.weight.data, 0.0, scaling)\n",
        "        elif classname.find('BatchNorm2d') != -1:  \n",
        "            torch.nn.init.normal_(m.weight.data, 1.0, scaling)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C84NXHNi0XS"
      },
      "source": [
        "# Downsample and Upsample Reusable code blocks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator model Architecture\n",
        "The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block.T\n",
        "```\n",
        "Encoder:  C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\n",
        "Decoder:  CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
        "```"
      ],
      "metadata": {
        "id": "Lvm8qIC_5exT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o0agkRqfiuhm"
      },
      "outputs": [],
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=False)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=False)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=False)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:   # add skip connections\n",
        "            return torch.cat([x, self.model(x)], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YaKADIMgiov1"
      },
      "outputs": [],
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    \"\"\"Create a Unet-based generator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, nf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        # add the innermost block\n",
        "        unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True) \n",
        "        #print(unet_block)\n",
        "\n",
        "        # add intermediate block with nf * 8 filters\n",
        "        unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(nf * 8, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "\n",
        "        # gradually reduce the number of filters from nf * 8 to nf. \n",
        "        unet_block = UnetSkipConnectionBlock(nf * 4, nf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(nf * 2, nf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(nf, nf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        \n",
        "        # add the outermost block\n",
        "        self.model = UnetSkipConnectionBlock(output_nc, nf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  \n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnmQw4w0i3mH"
      },
      "source": [
        "##create the generator architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCpf5D3biuDZ"
      },
      "outputs": [],
      "source": [
        "norm_layer=nn.BatchNorm2d\n",
        "\n",
        "generator = UnetGenerator(3, 3, 64, norm_layer=norm_layer, use_dropout=False).cuda().float() # define the generator\n",
        "\n",
        "# intialize the weights\n",
        "init_weights(generator, 'normal', scaling=0.02)\n",
        "\n",
        "generator = torch.nn.DataParallel(generator,device_ids=[0])  # multi-GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator Archictecture\n",
        "\n",
        "PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix."
      ],
      "metadata": {
        "id": "zcUIFWda5uBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8gbqc_8jWX3"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=False),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=False),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw), nn.Sigmoid()]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator intialisation"
      ],
      "metadata": {
        "id": "tXC6q-6n5xIP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyXwAr9TkiZN"
      },
      "outputs": [],
      "source": [
        "\n",
        "discriminator = Discriminator( 6, 64, norm_layer=norm_layer).cuda().float()\n",
        "init_weights(discriminator, 'normal', scaling=0.02)\n",
        "\n",
        "generator = torch.nn.DataParallel(generator,device_ids=[0,2,3])  # multi-GPUs\n",
        "discriminator = torch.nn.DataParallel(discriminator,device_ids=[0,2,3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3n-XOtkAe5"
      },
      "source": [
        "# loss functions\n",
        "\n",
        "\n",
        "Loss function used in Pix2Pix are `Adversarial loss` and  `Reconstruction loss`. \n",
        "- Adversarial loss is used to penalize the generator to predict more realistic images.\n",
        "- Reconstruction Loss helps network to produce the realistic image near the conditional image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I65D0bwj_rW"
      },
      "outputs": [],
      "source": [
        "adversarial_loss = nn.BCELoss()  # resposible for \n",
        "l1_loss = nn.L1Loss() # responsible for the pixel loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generator loss function"
      ],
      "metadata": {
        "id": "BXFyi4EY6BNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFDRBZ0ckEfr"
      },
      "outputs": [],
      "source": [
        "def generator_loss(generated_image, target_img, G, real_target):\n",
        "    gen_loss = adversarial_loss(G, real_target)\n",
        "    l1_l = l1_loss(generated_image, target_img)\n",
        "    gen_total_loss = gen_loss + (100 * l1_l)\n",
        "    #print(gen_loss)\n",
        "    return gen_total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## discriminator loss function"
      ],
      "metadata": {
        "id": "wkCGpV946D3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrLkhjckJon"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(output, label):\n",
        "    disc_loss = adversarial_loss(output, label)\n",
        "    return disc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuinKmdgp7QM"
      },
      "source": [
        "# save images and compute metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWXtTWI94Sle"
      },
      "outputs": [],
      "source": [
        "# compute the FID and IS scores\n",
        "def compute_evaluation_scores(real_img, fake_img,epoch):\n",
        "    # detacth the tensors \n",
        "    # convert the images to numpy arrays as the pytorch function requires numpy arrays\n",
        "    \n",
        "    real_t = torch.tensor(real_img.clone().detach().cpu().numpy(), dtype=torch.uint8) # real images tensor\n",
        "    genr_t = torch.tensor(fake_img.clone().detach().cpu().numpy(), dtype=torch.uint8) # generated images tensor\n",
        "    \n",
        "    # FID score\n",
        "    fid = FrechetInceptionDistance()\n",
        "    fid.update(real_t, real=True)\n",
        "    fid.update(genr_t, real=False)\n",
        "    fid_score = fid.compute()\n",
        "\n",
        "    # IS score\n",
        "    isc = InceptionScore()\n",
        "    isc.update(genr_t)\n",
        "    is_mean, is_std = isc.compute()\n",
        "\n",
        "    # log to tensorboard\n",
        "    writer.add_scalar('FID', fid_score, epoch)\n",
        "    writer.add_scalar('Inception_score_mean', is_mean, epoch)\n",
        "    writer.add_scalar('Inception_score_std', is_std, epoch)\n",
        "\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sNO4L-rp6om"
      },
      "outputs": [],
      "source": [
        "# save sample images after every epoch\n",
        "\n",
        "def save_samples(epoch=1):\n",
        "  for (inputs, targets), _ in valid_dl:\n",
        "\n",
        "    inputs = inputs.to(device) # move to device\n",
        "    targets = targets.to(device) # move to device\n",
        "    generated_output =  generator(inputs) # generate fake images\n",
        "\n",
        "    # compute the FID and IS scores and log \n",
        "    compute_evaluation_scores(targets, generated_output,epoch)\n",
        "\n",
        "    #Save the results\n",
        "    save_image(generated_output.data[:10], f\"./{samples_dir}/epoch_{epoch}.png\", nrow=2, normalize=True)  \n",
        "    # print images to notebook for every 10 epochs  \n",
        "    if(epoch%10 == 0):\n",
        "      im_grid = vutils.make_grid(generated_output.data[:9], padding = 2, normalize=True, nrow = 2).cpu()\n",
        "      plt.figure(figsize=(16,16))\n",
        "      plt.title(f'Generated at Epoch {epoch}')\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(np.transpose(im_grid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiqkwffSkNLr"
      },
      "source": [
        "# The Training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generator training"
      ],
      "metadata": {
        "id": "SUYWYs0i6VEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTbu0okKqq7f"
      },
      "outputs": [],
      "source": [
        "def train_generator(input_img,generated_image,real_target,G_optimizer):\n",
        "        G_optimizer.zero_grad()\n",
        "        fake_gen = torch.cat((input_img, generated_image), 1)\n",
        "        G = discriminator(fake_gen)\n",
        "        G_loss = generator_loss(generated_image, target_img, G, real_target)                                 \n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "        return G_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## discriminator training"
      ],
      "metadata": {
        "id": "ax-RtBRZ6XGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wg_-982p0SF"
      },
      "outputs": [],
      "source": [
        "def train_discrminator(input_img,generated_image,D_optimizer):\n",
        "        D_optimizer.zero_grad()\n",
        "        disc_inp_fake = torch.cat((input_img, generated_image), 1)\n",
        "        \n",
        "        D_fake = discriminator(disc_inp_fake.detach())\n",
        "        fake_target = torch.zeros_like(D_fake,device=device)\n",
        "\n",
        "        D_fake_loss   =  discriminator_loss(D_fake, fake_target)\n",
        "        \n",
        "        # train discriminator with real images\n",
        "        disc_inp_real = torch.cat((input_img, target_img), 1)\n",
        "                                \n",
        "        D_real = discriminator(disc_inp_real)\n",
        "        real_target = torch.ones_like(D_real, device=device)\n",
        "\n",
        "        D_real_loss = discriminator_loss(D_real,  real_target)\n",
        "\n",
        "    \n",
        "        # average discriminator loss\n",
        "        D_total_loss = (D_real_loss + D_fake_loss) / 2\n",
        "        \n",
        "\n",
        "        # compute gradients and run optimizer step\n",
        "        D_total_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        return real_target,D_total_loss,D_real_loss,D_fake_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create optimizers\n"
      ],
      "metadata": {
        "id": "lFKFxK7i6rCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "G_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "3BT6DO4X6qUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop\n"
      ],
      "metadata": {
        "id": "7ohS-W-f6aAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK21sMEbkOSf",
        "outputId": "d093aa13-d767-4065-9581-2ecbc9cf82b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total batches 1167\n",
            " Epoch[1|200] batch 200 Dloss:0.22185128927230835, gen_loss:34.09688949584961\n",
            " Epoch[1|200] batch 400 Dloss:0.8049044013023376, gen_loss:31.506221771240234\n",
            " Epoch[1|200] batch 600 Dloss:0.3417104184627533, gen_loss:34.749671936035156\n",
            " Epoch[1|200] batch 800 Dloss:0.4443960189819336, gen_loss:34.607643127441406\n",
            " Epoch[1|200] batch 1000 Dloss:0.2661190927028656, gen_loss:34.149234771728516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/common/home/vs734/anaconda3/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `FrechetInceptionDistance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/common/home/vs734/anaconda3/lib/python3.9/site-packages/scipy/linalg/_matfuncs_sqrtm.py:186: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  arg2 = norm(X.dot(X) - A, 'fro')**2 / norm(A, 'fro')\n",
            "/common/home/vs734/anaconda3/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch[2|200] batch 200 Dloss:0.3914801776409149, gen_loss:34.5770378112793\n",
            " Epoch[2|200] batch 400 Dloss:0.6520528793334961, gen_loss:32.659420013427734\n",
            " Epoch[2|200] batch 600 Dloss:0.48424726724624634, gen_loss:33.9962272644043\n",
            " Epoch[2|200] batch 800 Dloss:0.4167119860649109, gen_loss:34.2380256652832\n",
            " Epoch[2|200] batch 1000 Dloss:0.3682326674461365, gen_loss:34.51747512817383\n",
            " Epoch[3|200] batch 200 Dloss:0.32546916604042053, gen_loss:34.35558319091797\n",
            " Epoch[3|200] batch 400 Dloss:0.7584726810455322, gen_loss:33.00050735473633\n",
            " Epoch[3|200] batch 600 Dloss:0.5913808345794678, gen_loss:33.79820251464844\n",
            " Epoch[3|200] batch 800 Dloss:0.41057151556015015, gen_loss:33.115150451660156\n",
            " Epoch[3|200] batch 1000 Dloss:0.3092567026615143, gen_loss:33.850990295410156\n",
            " Epoch[4|200] batch 200 Dloss:0.32770437002182007, gen_loss:34.13134765625\n",
            " Epoch[4|200] batch 400 Dloss:1.0230777263641357, gen_loss:32.79178237915039\n",
            " Epoch[4|200] batch 600 Dloss:0.5300816297531128, gen_loss:33.58493423461914\n",
            " Epoch[4|200] batch 800 Dloss:0.36611321568489075, gen_loss:32.14918518066406\n",
            " Epoch[4|200] batch 1000 Dloss:0.2945769429206848, gen_loss:33.19684600830078\n",
            " Epoch[5|200] batch 200 Dloss:0.3587912917137146, gen_loss:33.5407829284668\n",
            " Epoch[5|200] batch 400 Dloss:0.8311046361923218, gen_loss:32.173709869384766\n",
            " Epoch[5|200] batch 600 Dloss:0.4804289937019348, gen_loss:32.711299896240234\n",
            " Epoch[5|200] batch 800 Dloss:0.3398779034614563, gen_loss:32.05452346801758\n",
            " Epoch[5|200] batch 1000 Dloss:0.3599128723144531, gen_loss:32.9620361328125\n",
            " Epoch[6|200] batch 200 Dloss:0.3690067529678345, gen_loss:33.151344299316406\n",
            " Epoch[6|200] batch 400 Dloss:0.7973235845565796, gen_loss:32.36194610595703\n",
            " Epoch[6|200] batch 600 Dloss:0.4532249867916107, gen_loss:32.639373779296875\n",
            " Epoch[6|200] batch 800 Dloss:0.35854482650756836, gen_loss:31.073749542236328\n",
            " Epoch[6|200] batch 1000 Dloss:0.3263651728630066, gen_loss:32.161746978759766\n",
            " Epoch[7|200] batch 200 Dloss:0.3891253173351288, gen_loss:32.60143280029297\n",
            " Epoch[7|200] batch 400 Dloss:0.769321084022522, gen_loss:31.39316749572754\n",
            " Epoch[7|200] batch 600 Dloss:0.5459080934524536, gen_loss:32.32501983642578\n",
            " Epoch[7|200] batch 800 Dloss:0.32357126474380493, gen_loss:30.68543243408203\n",
            " Epoch[7|200] batch 1000 Dloss:0.33548974990844727, gen_loss:31.926912307739258\n",
            " Epoch[8|200] batch 200 Dloss:0.34148508310317993, gen_loss:31.7016544342041\n",
            " Epoch[8|200] batch 400 Dloss:0.8487921953201294, gen_loss:31.588533401489258\n",
            " Epoch[8|200] batch 600 Dloss:0.5482762455940247, gen_loss:31.55978012084961\n",
            " Epoch[8|200] batch 800 Dloss:0.32678622007369995, gen_loss:30.540401458740234\n",
            " Epoch[8|200] batch 1000 Dloss:0.41823193430900574, gen_loss:31.272926330566406\n",
            " Epoch[9|200] batch 200 Dloss:0.313527375459671, gen_loss:31.44268035888672\n",
            " Epoch[9|200] batch 400 Dloss:0.8701390027999878, gen_loss:30.587074279785156\n",
            " Epoch[9|200] batch 600 Dloss:0.5275408029556274, gen_loss:31.241289138793945\n",
            " Epoch[9|200] batch 800 Dloss:0.3276764452457428, gen_loss:29.626623153686523\n",
            " Epoch[9|200] batch 1000 Dloss:0.4039340317249298, gen_loss:30.265071868896484\n",
            " Epoch[10|200] batch 200 Dloss:0.3625607192516327, gen_loss:31.176982879638672\n",
            " Epoch[10|200] batch 400 Dloss:0.7886884212493896, gen_loss:29.317174911499023\n",
            " Epoch[10|200] batch 600 Dloss:0.5089032649993896, gen_loss:30.091724395751953\n",
            " Epoch[10|200] batch 800 Dloss:0.3411388397216797, gen_loss:29.08755874633789\n",
            " Epoch[10|200] batch 1000 Dloss:0.418344646692276, gen_loss:29.877849578857422\n",
            " Epoch[11|200] batch 200 Dloss:0.3555336892604828, gen_loss:30.227161407470703\n",
            " Epoch[11|200] batch 400 Dloss:0.716779351234436, gen_loss:28.89088249206543\n",
            " Epoch[11|200] batch 600 Dloss:0.6397981643676758, gen_loss:29.23265838623047\n",
            " Epoch[11|200] batch 800 Dloss:0.35836923122406006, gen_loss:28.519805908203125\n",
            " Epoch[11|200] batch 1000 Dloss:0.5096481442451477, gen_loss:29.28028106689453\n",
            " Epoch[12|200] batch 200 Dloss:0.37197721004486084, gen_loss:30.157337188720703\n",
            " Epoch[12|200] batch 400 Dloss:0.8104819059371948, gen_loss:28.820009231567383\n",
            " Epoch[12|200] batch 600 Dloss:0.5624665021896362, gen_loss:27.692007064819336\n",
            " Epoch[12|200] batch 800 Dloss:0.3869767189025879, gen_loss:26.819456100463867\n",
            " Epoch[12|200] batch 1000 Dloss:0.5021254420280457, gen_loss:28.56830406188965\n",
            " Epoch[13|200] batch 200 Dloss:0.37554898858070374, gen_loss:28.679357528686523\n",
            " Epoch[13|200] batch 400 Dloss:0.7191755771636963, gen_loss:27.47852325439453\n",
            " Epoch[13|200] batch 600 Dloss:0.4970615804195404, gen_loss:27.05803108215332\n",
            " Epoch[13|200] batch 800 Dloss:0.387908935546875, gen_loss:27.241289138793945\n",
            " Epoch[13|200] batch 1000 Dloss:0.6151959896087646, gen_loss:28.574447631835938\n",
            " Epoch[14|200] batch 200 Dloss:0.4147804379463196, gen_loss:27.967845916748047\n",
            " Epoch[14|200] batch 400 Dloss:0.6732490062713623, gen_loss:26.997516632080078\n",
            " Epoch[14|200] batch 600 Dloss:0.48034536838531494, gen_loss:25.797683715820312\n",
            " Epoch[14|200] batch 800 Dloss:0.34831124544143677, gen_loss:25.952327728271484\n",
            " Epoch[14|200] batch 1000 Dloss:0.5101892948150635, gen_loss:27.61941909790039\n",
            " Epoch[15|200] batch 200 Dloss:0.3745664358139038, gen_loss:27.951139450073242\n",
            " Epoch[15|200] batch 400 Dloss:0.7540237903594971, gen_loss:26.554027557373047\n",
            " Epoch[15|200] batch 600 Dloss:0.5455314517021179, gen_loss:25.435413360595703\n",
            " Epoch[15|200] batch 800 Dloss:0.3939322829246521, gen_loss:25.263181686401367\n",
            " Epoch[15|200] batch 1000 Dloss:0.5532891154289246, gen_loss:26.612869262695312\n",
            " Epoch[16|200] batch 200 Dloss:0.37276530265808105, gen_loss:27.022703170776367\n",
            " Epoch[16|200] batch 400 Dloss:0.6619318723678589, gen_loss:26.15597915649414\n",
            " Epoch[16|200] batch 600 Dloss:0.4929502606391907, gen_loss:25.627721786499023\n",
            " Epoch[16|200] batch 800 Dloss:0.3517436385154724, gen_loss:24.77996063232422\n",
            " Epoch[16|200] batch 1000 Dloss:0.6005123853683472, gen_loss:26.9327449798584\n",
            " Epoch[17|200] batch 200 Dloss:0.3716406226158142, gen_loss:26.666000366210938\n",
            " Epoch[17|200] batch 400 Dloss:0.6260929107666016, gen_loss:25.19278335571289\n",
            " Epoch[17|200] batch 600 Dloss:0.4684973955154419, gen_loss:24.64389991760254\n",
            " Epoch[17|200] batch 800 Dloss:0.3909861743450165, gen_loss:24.186838150024414\n",
            " Epoch[17|200] batch 1000 Dloss:0.5268757939338684, gen_loss:25.941162109375\n",
            " Epoch[18|200] batch 200 Dloss:0.3480418920516968, gen_loss:25.791526794433594\n",
            " Epoch[18|200] batch 400 Dloss:0.6238318681716919, gen_loss:24.67474365234375\n",
            " Epoch[18|200] batch 600 Dloss:0.4510217308998108, gen_loss:23.970882415771484\n",
            " Epoch[18|200] batch 800 Dloss:0.396924763917923, gen_loss:23.139362335205078\n",
            " Epoch[18|200] batch 1000 Dloss:0.6354467868804932, gen_loss:25.015600204467773\n",
            " Epoch[19|200] batch 200 Dloss:0.39978519082069397, gen_loss:25.611003875732422\n",
            " Epoch[19|200] batch 400 Dloss:0.5925944447517395, gen_loss:24.2076473236084\n",
            " Epoch[19|200] batch 600 Dloss:0.43238624930381775, gen_loss:23.68226432800293\n",
            " Epoch[19|200] batch 800 Dloss:0.3520798981189728, gen_loss:22.707901000976562\n",
            " Epoch[19|200] batch 1000 Dloss:0.5189803242683411, gen_loss:25.162805557250977\n",
            " Epoch[20|200] batch 200 Dloss:0.3807549476623535, gen_loss:24.353008270263672\n",
            " Epoch[20|200] batch 400 Dloss:0.5589989423751831, gen_loss:24.022907257080078\n",
            " Epoch[20|200] batch 600 Dloss:0.4273762106895447, gen_loss:23.235103607177734\n",
            " Epoch[20|200] batch 800 Dloss:0.3750494122505188, gen_loss:22.942731857299805\n",
            " Epoch[20|200] batch 1000 Dloss:0.6946507096290588, gen_loss:24.111835479736328\n",
            " Epoch[21|200] batch 200 Dloss:0.40883904695510864, gen_loss:24.31202507019043\n",
            " Epoch[21|200] batch 400 Dloss:0.5298383235931396, gen_loss:23.66710662841797\n",
            " Epoch[21|200] batch 600 Dloss:0.4705148935317993, gen_loss:23.022254943847656\n",
            " Epoch[21|200] batch 800 Dloss:0.43566691875457764, gen_loss:22.503129959106445\n",
            " Epoch[21|200] batch 1000 Dloss:0.611093282699585, gen_loss:23.90993309020996\n",
            " Epoch[22|200] batch 200 Dloss:0.43329375982284546, gen_loss:23.557817459106445\n",
            " Epoch[22|200] batch 400 Dloss:0.5960786938667297, gen_loss:23.039461135864258\n",
            " Epoch[22|200] batch 600 Dloss:0.4246826171875, gen_loss:22.32019805908203\n",
            " Epoch[22|200] batch 800 Dloss:0.3835859000682831, gen_loss:21.81755256652832\n",
            " Epoch[22|200] batch 1000 Dloss:0.5138692259788513, gen_loss:23.250619888305664\n",
            " Epoch[23|200] batch 200 Dloss:0.42000874876976013, gen_loss:23.40067481994629\n",
            " Epoch[23|200] batch 400 Dloss:0.6909817457199097, gen_loss:23.826162338256836\n",
            " Epoch[23|200] batch 600 Dloss:0.47023874521255493, gen_loss:21.998470306396484\n",
            " Epoch[23|200] batch 800 Dloss:0.3830763101577759, gen_loss:21.574308395385742\n",
            " Epoch[23|200] batch 1000 Dloss:0.48007723689079285, gen_loss:22.417503356933594\n",
            " Epoch[24|200] batch 200 Dloss:0.42223024368286133, gen_loss:23.111303329467773\n",
            " Epoch[24|200] batch 400 Dloss:0.5371223092079163, gen_loss:22.5800724029541\n",
            " Epoch[24|200] batch 600 Dloss:0.45065945386886597, gen_loss:21.270771026611328\n",
            " Epoch[24|200] batch 800 Dloss:0.36411052942276, gen_loss:21.253372192382812\n",
            " Epoch[24|200] batch 1000 Dloss:0.5432351231575012, gen_loss:22.611095428466797\n",
            " Epoch[25|200] batch 200 Dloss:0.41041409969329834, gen_loss:22.541921615600586\n",
            " Epoch[25|200] batch 400 Dloss:0.526097297668457, gen_loss:22.309499740600586\n",
            " Epoch[25|200] batch 600 Dloss:0.43914252519607544, gen_loss:22.280025482177734\n",
            " Epoch[25|200] batch 800 Dloss:0.37010446190834045, gen_loss:20.89131736755371\n",
            " Epoch[25|200] batch 1000 Dloss:0.5206595063209534, gen_loss:22.133655548095703\n",
            " Epoch[26|200] batch 200 Dloss:0.36502742767333984, gen_loss:22.834110260009766\n",
            " Epoch[26|200] batch 400 Dloss:0.5182769298553467, gen_loss:22.356670379638672\n",
            " Epoch[26|200] batch 600 Dloss:0.4859936237335205, gen_loss:21.288969039916992\n",
            " Epoch[26|200] batch 800 Dloss:0.4316002130508423, gen_loss:20.402496337890625\n",
            " Epoch[26|200] batch 1000 Dloss:0.612468421459198, gen_loss:21.993005752563477\n",
            " Epoch[27|200] batch 200 Dloss:0.40942490100860596, gen_loss:22.080020904541016\n",
            " Epoch[27|200] batch 400 Dloss:0.5560174584388733, gen_loss:21.702720642089844\n",
            " Epoch[27|200] batch 600 Dloss:0.4610089957714081, gen_loss:21.20021629333496\n",
            " Epoch[27|200] batch 800 Dloss:0.4127204120159149, gen_loss:20.525821685791016\n",
            " Epoch[27|200] batch 1000 Dloss:0.47064724564552307, gen_loss:21.275577545166016\n",
            " Epoch[28|200] batch 200 Dloss:0.41005659103393555, gen_loss:21.81308364868164\n",
            " Epoch[28|200] batch 400 Dloss:0.5256693959236145, gen_loss:21.68498992919922\n",
            " Epoch[28|200] batch 600 Dloss:0.4840172529220581, gen_loss:21.00674057006836\n",
            " Epoch[28|200] batch 800 Dloss:0.3388836681842804, gen_loss:20.492897033691406\n",
            " Epoch[28|200] batch 1000 Dloss:0.5189695358276367, gen_loss:21.213993072509766\n",
            " Epoch[29|200] batch 200 Dloss:0.3597889840602875, gen_loss:21.84795379638672\n",
            " Epoch[29|200] batch 400 Dloss:0.543277382850647, gen_loss:21.36620330810547\n",
            " Epoch[29|200] batch 600 Dloss:0.46441560983657837, gen_loss:20.62224006652832\n",
            " Epoch[29|200] batch 800 Dloss:0.4059857130050659, gen_loss:20.242374420166016\n",
            " Epoch[29|200] batch 1000 Dloss:0.4208403527736664, gen_loss:20.83858299255371\n",
            " Epoch[30|200] batch 200 Dloss:0.37438344955444336, gen_loss:21.20594596862793\n",
            " Epoch[30|200] batch 400 Dloss:0.5393335819244385, gen_loss:21.37251853942871\n",
            " Epoch[30|200] batch 600 Dloss:0.5168236494064331, gen_loss:20.513229370117188\n",
            " Epoch[30|200] batch 800 Dloss:0.3953828811645508, gen_loss:19.866275787353516\n",
            " Epoch[30|200] batch 1000 Dloss:0.5307508707046509, gen_loss:21.562267303466797\n",
            " Epoch[31|200] batch 200 Dloss:0.36225029826164246, gen_loss:21.104087829589844\n",
            " Epoch[31|200] batch 400 Dloss:0.4924444556236267, gen_loss:20.95914649963379\n",
            " Epoch[31|200] batch 600 Dloss:0.5025289058685303, gen_loss:20.83656120300293\n",
            " Epoch[31|200] batch 800 Dloss:0.35505276918411255, gen_loss:19.560447692871094\n",
            " Epoch[31|200] batch 1000 Dloss:0.4788253605365753, gen_loss:20.218799591064453\n",
            " Epoch[32|200] batch 200 Dloss:0.3692723512649536, gen_loss:21.221561431884766\n",
            " Epoch[32|200] batch 400 Dloss:0.4543624520301819, gen_loss:20.751562118530273\n",
            " Epoch[32|200] batch 600 Dloss:0.46088796854019165, gen_loss:19.914079666137695\n",
            " Epoch[32|200] batch 800 Dloss:0.36456865072250366, gen_loss:19.175790786743164\n",
            " Epoch[32|200] batch 1000 Dloss:0.526593804359436, gen_loss:20.76770782470703\n",
            " Epoch[33|200] batch 200 Dloss:0.3418014645576477, gen_loss:21.004514694213867\n",
            " Epoch[33|200] batch 400 Dloss:0.5872923135757446, gen_loss:20.68730926513672\n",
            " Epoch[33|200] batch 600 Dloss:0.4665234088897705, gen_loss:19.962867736816406\n",
            " Epoch[33|200] batch 800 Dloss:0.3825683891773224, gen_loss:19.418882369995117\n",
            " Epoch[33|200] batch 1000 Dloss:0.48457399010658264, gen_loss:19.95447540283203\n",
            " Epoch[34|200] batch 200 Dloss:0.32430267333984375, gen_loss:20.538312911987305\n",
            " Epoch[34|200] batch 400 Dloss:0.4547477960586548, gen_loss:20.1374454498291\n",
            " Epoch[34|200] batch 600 Dloss:0.46818479895591736, gen_loss:19.89480972290039\n",
            " Epoch[34|200] batch 800 Dloss:0.3962325155735016, gen_loss:19.36217498779297\n",
            " Epoch[34|200] batch 1000 Dloss:0.5243574380874634, gen_loss:20.03961753845215\n",
            " Epoch[35|200] batch 200 Dloss:0.37686067819595337, gen_loss:20.878677368164062\n",
            " Epoch[35|200] batch 400 Dloss:0.45067447423934937, gen_loss:20.444927215576172\n",
            " Epoch[35|200] batch 600 Dloss:0.5278311371803284, gen_loss:19.738819122314453\n",
            " Epoch[35|200] batch 800 Dloss:0.33459538221359253, gen_loss:18.527555465698242\n",
            " Epoch[35|200] batch 1000 Dloss:0.5762428641319275, gen_loss:20.10493278503418\n",
            " Epoch[36|200] batch 200 Dloss:0.40242233872413635, gen_loss:20.541078567504883\n",
            " Epoch[36|200] batch 400 Dloss:0.4656934440135956, gen_loss:19.851308822631836\n",
            " Epoch[36|200] batch 600 Dloss:0.452312707901001, gen_loss:19.602741241455078\n",
            " Epoch[36|200] batch 800 Dloss:0.38573890924453735, gen_loss:18.7036075592041\n",
            " Epoch[36|200] batch 1000 Dloss:0.41576167941093445, gen_loss:20.08901596069336\n",
            " Epoch[37|200] batch 200 Dloss:0.3342897295951843, gen_loss:20.115478515625\n",
            " Epoch[37|200] batch 400 Dloss:0.48719322681427, gen_loss:19.840656280517578\n",
            " Epoch[37|200] batch 600 Dloss:0.4542486071586609, gen_loss:19.757490158081055\n",
            " Epoch[37|200] batch 800 Dloss:0.33452585339546204, gen_loss:18.817089080810547\n",
            " Epoch[37|200] batch 1000 Dloss:0.5152620077133179, gen_loss:19.68245506286621\n",
            " Epoch[38|200] batch 200 Dloss:0.34997567534446716, gen_loss:20.103893280029297\n",
            " Epoch[38|200] batch 400 Dloss:0.4655238389968872, gen_loss:19.721242904663086\n",
            " Epoch[38|200] batch 600 Dloss:0.5306364893913269, gen_loss:19.630002975463867\n",
            " Epoch[38|200] batch 800 Dloss:0.3984529376029968, gen_loss:18.443965911865234\n",
            " Epoch[38|200] batch 1000 Dloss:0.4257654547691345, gen_loss:19.359956741333008\n",
            " Epoch[39|200] batch 200 Dloss:0.2925161123275757, gen_loss:19.818115234375\n",
            " Epoch[39|200] batch 400 Dloss:0.45910459756851196, gen_loss:19.591493606567383\n",
            " Epoch[39|200] batch 600 Dloss:0.5182291269302368, gen_loss:19.575271606445312\n",
            " Epoch[39|200] batch 800 Dloss:0.35131847858428955, gen_loss:18.4393253326416\n",
            " Epoch[39|200] batch 1000 Dloss:0.3871059715747833, gen_loss:19.959487915039062\n",
            " Epoch[40|200] batch 200 Dloss:0.33753618597984314, gen_loss:19.909568786621094\n",
            " Epoch[40|200] batch 400 Dloss:0.411437064409256, gen_loss:19.491046905517578\n",
            " Epoch[40|200] batch 600 Dloss:0.48605477809906006, gen_loss:19.53341293334961\n",
            " Epoch[40|200] batch 800 Dloss:0.31237876415252686, gen_loss:18.724960327148438\n",
            " Epoch[40|200] batch 1000 Dloss:0.4539872109889984, gen_loss:19.713228225708008\n",
            " Epoch[41|200] batch 200 Dloss:0.3832876980304718, gen_loss:20.3835506439209\n",
            " Epoch[41|200] batch 400 Dloss:0.3827977180480957, gen_loss:19.605798721313477\n",
            " Epoch[41|200] batch 600 Dloss:0.5075361728668213, gen_loss:19.49022674560547\n",
            " Epoch[41|200] batch 800 Dloss:0.2844431400299072, gen_loss:18.925615310668945\n",
            " Epoch[41|200] batch 1000 Dloss:0.30296027660369873, gen_loss:19.86813735961914\n",
            " Epoch[42|200] batch 200 Dloss:0.33225229382514954, gen_loss:20.555984497070312\n",
            " Epoch[42|200] batch 400 Dloss:0.5009658336639404, gen_loss:20.1854305267334\n",
            " Epoch[42|200] batch 600 Dloss:0.3812164068222046, gen_loss:19.565343856811523\n",
            " Epoch[42|200] batch 800 Dloss:0.2943183481693268, gen_loss:18.91462516784668\n",
            " Epoch[42|200] batch 1000 Dloss:0.5424321889877319, gen_loss:20.44106101989746\n",
            " Epoch[43|200] batch 200 Dloss:0.2034316211938858, gen_loss:19.985782623291016\n",
            " Epoch[43|200] batch 400 Dloss:0.4111870229244232, gen_loss:19.56342887878418\n",
            " Epoch[43|200] batch 600 Dloss:0.4021710455417633, gen_loss:19.393333435058594\n",
            " Epoch[43|200] batch 800 Dloss:0.30355215072631836, gen_loss:18.865381240844727\n",
            " Epoch[43|200] batch 1000 Dloss:0.32540035247802734, gen_loss:19.318574905395508\n",
            " Epoch[44|200] batch 200 Dloss:0.2537989616394043, gen_loss:20.05173110961914\n",
            " Epoch[44|200] batch 400 Dloss:0.3412218987941742, gen_loss:19.924331665039062\n",
            " Epoch[44|200] batch 600 Dloss:0.4492533504962921, gen_loss:19.46918487548828\n",
            " Epoch[44|200] batch 800 Dloss:0.3075481951236725, gen_loss:18.593915939331055\n",
            " Epoch[44|200] batch 1000 Dloss:0.3270130157470703, gen_loss:19.625972747802734\n",
            " Epoch[45|200] batch 200 Dloss:0.23783224821090698, gen_loss:20.325166702270508\n",
            " Epoch[45|200] batch 400 Dloss:0.2770054340362549, gen_loss:20.184921264648438\n",
            " Epoch[45|200] batch 600 Dloss:0.47145578265190125, gen_loss:19.641969680786133\n",
            " Epoch[45|200] batch 800 Dloss:0.2521117627620697, gen_loss:19.996816635131836\n",
            " Epoch[45|200] batch 1000 Dloss:0.3353862464427948, gen_loss:20.148637771606445\n",
            " Epoch[46|200] batch 200 Dloss:0.153135284781456, gen_loss:19.899198532104492\n",
            " Epoch[46|200] batch 400 Dloss:0.2992451786994934, gen_loss:20.288610458374023\n",
            " Epoch[46|200] batch 600 Dloss:0.37070757150650024, gen_loss:19.412399291992188\n",
            " Epoch[46|200] batch 800 Dloss:0.2412509024143219, gen_loss:19.39104652404785\n",
            " Epoch[46|200] batch 1000 Dloss:0.2565821707248688, gen_loss:19.593421936035156\n",
            " Epoch[47|200] batch 200 Dloss:0.2344476729631424, gen_loss:20.238317489624023\n",
            " Epoch[47|200] batch 400 Dloss:0.34165656566619873, gen_loss:20.438796997070312\n",
            " Epoch[47|200] batch 600 Dloss:0.3965863287448883, gen_loss:19.638683319091797\n",
            " Epoch[47|200] batch 800 Dloss:0.30135107040405273, gen_loss:19.42376708984375\n",
            " Epoch[47|200] batch 1000 Dloss:0.26978403329849243, gen_loss:19.87603187561035\n",
            " Epoch[48|200] batch 200 Dloss:0.20407864451408386, gen_loss:20.235639572143555\n",
            " Epoch[48|200] batch 400 Dloss:0.3141050338745117, gen_loss:19.724361419677734\n",
            " Epoch[48|200] batch 600 Dloss:0.350879430770874, gen_loss:19.19748306274414\n",
            " Epoch[48|200] batch 800 Dloss:0.28576216101646423, gen_loss:19.181865692138672\n",
            " Epoch[48|200] batch 1000 Dloss:0.30054983496665955, gen_loss:19.24260711669922\n",
            " Epoch[49|200] batch 200 Dloss:0.20935794711112976, gen_loss:20.216129302978516\n",
            " Epoch[49|200] batch 400 Dloss:0.32912132143974304, gen_loss:19.94218635559082\n",
            " Epoch[49|200] batch 600 Dloss:0.33792927861213684, gen_loss:19.318973541259766\n",
            " Epoch[49|200] batch 800 Dloss:0.2767798602581024, gen_loss:19.551950454711914\n",
            " Epoch[49|200] batch 1000 Dloss:0.34925591945648193, gen_loss:19.43671417236328\n",
            " Epoch[50|200] batch 200 Dloss:0.2130664885044098, gen_loss:20.74095344543457\n",
            " Epoch[50|200] batch 400 Dloss:0.3078298568725586, gen_loss:19.987661361694336\n",
            " Epoch[50|200] batch 600 Dloss:0.41407260298728943, gen_loss:19.678844451904297\n",
            " Epoch[50|200] batch 800 Dloss:0.2641083002090454, gen_loss:19.62441062927246\n",
            " Epoch[50|200] batch 1000 Dloss:0.29967936873435974, gen_loss:19.676475524902344\n",
            " Epoch[51|200] batch 200 Dloss:0.18511296808719635, gen_loss:19.57166290283203\n",
            " Epoch[51|200] batch 400 Dloss:0.3382513225078583, gen_loss:20.026273727416992\n",
            " Epoch[51|200] batch 600 Dloss:0.3059805929660797, gen_loss:19.42424201965332\n",
            " Epoch[51|200] batch 800 Dloss:0.23742792010307312, gen_loss:19.418048858642578\n",
            " Epoch[51|200] batch 1000 Dloss:0.2774121165275574, gen_loss:19.772085189819336\n",
            " Epoch[52|200] batch 200 Dloss:0.21600642800331116, gen_loss:20.237998962402344\n",
            " Epoch[52|200] batch 400 Dloss:0.31230780482292175, gen_loss:19.60091781616211\n",
            " Epoch[52|200] batch 600 Dloss:0.38330453634262085, gen_loss:19.68570327758789\n",
            " Epoch[52|200] batch 800 Dloss:0.233134925365448, gen_loss:19.672138214111328\n",
            " Epoch[52|200] batch 1000 Dloss:0.28056085109710693, gen_loss:19.721342086791992\n",
            " Epoch[53|200] batch 200 Dloss:0.25995105504989624, gen_loss:20.455167770385742\n",
            " Epoch[53|200] batch 400 Dloss:0.3459867537021637, gen_loss:19.7641658782959\n",
            " Epoch[53|200] batch 600 Dloss:0.2756350636482239, gen_loss:19.55930519104004\n",
            " Epoch[53|200] batch 800 Dloss:0.2771318256855011, gen_loss:19.288211822509766\n",
            " Epoch[53|200] batch 1000 Dloss:0.23966312408447266, gen_loss:19.757877349853516\n",
            " Epoch[54|200] batch 200 Dloss:0.16742946207523346, gen_loss:20.227874755859375\n",
            " Epoch[54|200] batch 400 Dloss:0.3318064212799072, gen_loss:19.937835693359375\n",
            " Epoch[54|200] batch 600 Dloss:0.23507115244865417, gen_loss:19.51068115234375\n",
            " Epoch[54|200] batch 800 Dloss:0.2731742560863495, gen_loss:19.630590438842773\n",
            " Epoch[54|200] batch 1000 Dloss:0.2834225594997406, gen_loss:19.312204360961914\n",
            " Epoch[55|200] batch 200 Dloss:0.19859810173511505, gen_loss:19.942981719970703\n",
            " Epoch[55|200] batch 400 Dloss:0.30218374729156494, gen_loss:19.78183364868164\n",
            " Epoch[55|200] batch 600 Dloss:0.4017757177352905, gen_loss:19.224992752075195\n",
            " Epoch[55|200] batch 800 Dloss:0.23424792289733887, gen_loss:19.157100677490234\n",
            " Epoch[55|200] batch 1000 Dloss:0.2691548764705658, gen_loss:19.35015869140625\n",
            " Epoch[56|200] batch 200 Dloss:0.16465704143047333, gen_loss:19.943523406982422\n",
            " Epoch[56|200] batch 400 Dloss:0.371132493019104, gen_loss:19.762073516845703\n",
            " Epoch[56|200] batch 600 Dloss:0.256674587726593, gen_loss:19.748523712158203\n",
            " Epoch[56|200] batch 800 Dloss:0.24392712116241455, gen_loss:19.245145797729492\n",
            " Epoch[56|200] batch 1000 Dloss:0.27717626094818115, gen_loss:19.299022674560547\n",
            " Epoch[57|200] batch 200 Dloss:0.18215472996234894, gen_loss:19.653427124023438\n",
            " Epoch[57|200] batch 400 Dloss:0.35507044196128845, gen_loss:20.358156204223633\n",
            " Epoch[57|200] batch 600 Dloss:0.23065386712551117, gen_loss:19.471769332885742\n",
            " Epoch[57|200] batch 800 Dloss:0.4184713661670685, gen_loss:19.23819351196289\n",
            " Epoch[57|200] batch 1000 Dloss:0.29846900701522827, gen_loss:19.32516860961914\n",
            " Epoch[58|200] batch 200 Dloss:0.1776767075061798, gen_loss:20.38568878173828\n",
            " Epoch[58|200] batch 400 Dloss:0.3814135789871216, gen_loss:19.968666076660156\n",
            " Epoch[58|200] batch 600 Dloss:0.2255312204360962, gen_loss:19.551992416381836\n",
            " Epoch[58|200] batch 800 Dloss:0.25171196460723877, gen_loss:19.0202693939209\n",
            " Epoch[58|200] batch 1000 Dloss:0.2754780948162079, gen_loss:19.366918563842773\n",
            " Epoch[59|200] batch 200 Dloss:0.16097791492938995, gen_loss:20.234237670898438\n",
            " Epoch[59|200] batch 400 Dloss:0.3326437175273895, gen_loss:20.34696388244629\n",
            " Epoch[59|200] batch 600 Dloss:0.28854650259017944, gen_loss:19.52088737487793\n",
            " Epoch[59|200] batch 800 Dloss:0.2626258432865143, gen_loss:19.72962760925293\n",
            " Epoch[59|200] batch 1000 Dloss:0.3277628421783447, gen_loss:19.2314453125\n",
            " Epoch[60|200] batch 200 Dloss:0.16844210028648376, gen_loss:20.455848693847656\n",
            " Epoch[60|200] batch 400 Dloss:0.29935574531555176, gen_loss:19.93443489074707\n",
            " Epoch[60|200] batch 600 Dloss:0.27806711196899414, gen_loss:19.472097396850586\n",
            " Epoch[60|200] batch 800 Dloss:0.19689369201660156, gen_loss:19.82073974609375\n",
            " Epoch[60|200] batch 1000 Dloss:1.8787221908569336, gen_loss:19.755775451660156\n",
            " Epoch[61|200] batch 200 Dloss:0.27809253334999084, gen_loss:20.025022506713867\n",
            " Epoch[61|200] batch 400 Dloss:0.2725096046924591, gen_loss:19.896875381469727\n",
            " Epoch[61|200] batch 600 Dloss:0.437264084815979, gen_loss:19.337656021118164\n",
            " Epoch[61|200] batch 800 Dloss:0.2584264874458313, gen_loss:19.80437469482422\n",
            " Epoch[61|200] batch 1000 Dloss:0.25821858644485474, gen_loss:19.019821166992188\n",
            " Epoch[62|200] batch 200 Dloss:0.15765780210494995, gen_loss:19.92379379272461\n",
            " Epoch[62|200] batch 400 Dloss:0.25293514132499695, gen_loss:20.351089477539062\n",
            " Epoch[62|200] batch 600 Dloss:0.24726808071136475, gen_loss:19.454435348510742\n",
            " Epoch[62|200] batch 800 Dloss:0.2106969803571701, gen_loss:19.36614418029785\n",
            " Epoch[62|200] batch 1000 Dloss:0.27065005898475647, gen_loss:19.067598342895508\n",
            " Epoch[63|200] batch 200 Dloss:1.836284875869751, gen_loss:22.798383712768555\n",
            " Epoch[63|200] batch 400 Dloss:0.25892895460128784, gen_loss:19.546510696411133\n",
            " Epoch[63|200] batch 600 Dloss:0.2763720452785492, gen_loss:19.58870506286621\n",
            " Epoch[63|200] batch 800 Dloss:0.2517741322517395, gen_loss:19.38550567626953\n",
            " Epoch[63|200] batch 1000 Dloss:0.2541597783565521, gen_loss:19.670181274414062\n",
            " Epoch[64|200] batch 200 Dloss:0.2574847638607025, gen_loss:19.655517578125\n",
            " Epoch[64|200] batch 400 Dloss:0.2867870330810547, gen_loss:20.66031265258789\n",
            " Epoch[64|200] batch 600 Dloss:0.17286112904548645, gen_loss:19.634244918823242\n",
            " Epoch[64|200] batch 800 Dloss:0.17790326476097107, gen_loss:20.406370162963867\n",
            " Epoch[64|200] batch 1000 Dloss:0.30062082409858704, gen_loss:19.085168838500977\n",
            " Epoch[65|200] batch 200 Dloss:0.1565217673778534, gen_loss:20.016042709350586\n",
            " Epoch[65|200] batch 400 Dloss:0.28189775347709656, gen_loss:19.686147689819336\n",
            " Epoch[65|200] batch 600 Dloss:0.3526230454444885, gen_loss:19.946083068847656\n",
            " Epoch[65|200] batch 800 Dloss:0.3449625074863434, gen_loss:19.34943962097168\n",
            " Epoch[65|200] batch 1000 Dloss:0.27903351187705994, gen_loss:19.95648765563965\n",
            " Epoch[66|200] batch 200 Dloss:0.16547633707523346, gen_loss:20.225149154663086\n",
            " Epoch[66|200] batch 400 Dloss:0.2804296910762787, gen_loss:20.379138946533203\n",
            " Epoch[66|200] batch 600 Dloss:0.1868036687374115, gen_loss:19.852170944213867\n",
            " Epoch[66|200] batch 800 Dloss:0.20479664206504822, gen_loss:19.751100540161133\n",
            " Epoch[66|200] batch 1000 Dloss:0.25571829080581665, gen_loss:19.530502319335938\n",
            " Epoch[67|200] batch 200 Dloss:0.2141016721725464, gen_loss:21.183944702148438\n",
            " Epoch[67|200] batch 400 Dloss:0.22754818201065063, gen_loss:19.51568603515625\n",
            " Epoch[67|200] batch 600 Dloss:0.2682822644710541, gen_loss:19.484323501586914\n",
            " Epoch[67|200] batch 800 Dloss:0.39243608713150024, gen_loss:20.204601287841797\n",
            " Epoch[67|200] batch 1000 Dloss:0.3085276782512665, gen_loss:19.61756134033203\n",
            " Epoch[68|200] batch 200 Dloss:0.13016140460968018, gen_loss:20.70892906188965\n",
            " Epoch[68|200] batch 400 Dloss:0.2879171073436737, gen_loss:20.458267211914062\n",
            " Epoch[68|200] batch 600 Dloss:0.2826741933822632, gen_loss:19.143014907836914\n",
            " Epoch[68|200] batch 800 Dloss:0.4435235261917114, gen_loss:20.253206253051758\n",
            " Epoch[68|200] batch 1000 Dloss:0.18960808217525482, gen_loss:19.568153381347656\n",
            " Epoch[69|200] batch 200 Dloss:0.1923476755619049, gen_loss:20.445341110229492\n",
            " Epoch[69|200] batch 400 Dloss:0.2368883490562439, gen_loss:19.787992477416992\n",
            " Epoch[69|200] batch 600 Dloss:0.2003573477268219, gen_loss:19.280038833618164\n",
            " Epoch[69|200] batch 800 Dloss:0.21334496140480042, gen_loss:20.348281860351562\n",
            " Epoch[69|200] batch 1000 Dloss:0.2973746359348297, gen_loss:18.949512481689453\n",
            " Epoch[70|200] batch 200 Dloss:0.25157779455184937, gen_loss:19.97821617126465\n",
            " Epoch[70|200] batch 400 Dloss:0.26226896047592163, gen_loss:20.767005920410156\n",
            " Epoch[70|200] batch 600 Dloss:0.15695807337760925, gen_loss:19.910232543945312\n",
            " Epoch[70|200] batch 800 Dloss:0.13114285469055176, gen_loss:19.420257568359375\n",
            " Epoch[70|200] batch 1000 Dloss:0.2892839312553406, gen_loss:19.431625366210938\n",
            " Epoch[71|200] batch 200 Dloss:0.13630615174770355, gen_loss:19.93470573425293\n",
            " Epoch[71|200] batch 400 Dloss:0.1913183033466339, gen_loss:19.920034408569336\n",
            " Epoch[71|200] batch 600 Dloss:0.20784975588321686, gen_loss:20.932479858398438\n",
            " Epoch[71|200] batch 800 Dloss:0.18994568288326263, gen_loss:20.18904685974121\n",
            " Epoch[71|200] batch 1000 Dloss:0.43180975317955017, gen_loss:18.750377655029297\n",
            " Epoch[72|200] batch 200 Dloss:0.1466759741306305, gen_loss:20.82947540283203\n",
            " Epoch[72|200] batch 400 Dloss:0.21387577056884766, gen_loss:19.811288833618164\n",
            " Epoch[72|200] batch 600 Dloss:0.22227787971496582, gen_loss:19.73386001586914\n",
            " Epoch[72|200] batch 800 Dloss:0.1378830373287201, gen_loss:19.994516372680664\n",
            " Epoch[72|200] batch 1000 Dloss:0.2960870862007141, gen_loss:19.37610626220703\n",
            " Epoch[73|200] batch 200 Dloss:0.11832475662231445, gen_loss:20.157093048095703\n",
            " Epoch[73|200] batch 400 Dloss:0.2520214319229126, gen_loss:20.046052932739258\n",
            " Epoch[73|200] batch 600 Dloss:0.17962674796581268, gen_loss:19.964622497558594\n",
            " Epoch[73|200] batch 800 Dloss:0.18688297271728516, gen_loss:20.193126678466797\n",
            " Epoch[73|200] batch 1000 Dloss:0.4782719016075134, gen_loss:19.34832000732422\n",
            " Epoch[74|200] batch 200 Dloss:0.12076415121555328, gen_loss:20.29722023010254\n",
            " Epoch[74|200] batch 400 Dloss:0.26737335324287415, gen_loss:20.074066162109375\n",
            " Epoch[74|200] batch 600 Dloss:0.21463604271411896, gen_loss:19.836994171142578\n",
            " Epoch[74|200] batch 800 Dloss:0.12336008250713348, gen_loss:19.81545066833496\n",
            " Epoch[74|200] batch 1000 Dloss:0.22445501387119293, gen_loss:19.55178451538086\n",
            " Epoch[75|200] batch 200 Dloss:0.13777285814285278, gen_loss:19.952701568603516\n",
            " Epoch[75|200] batch 400 Dloss:0.23013457655906677, gen_loss:19.64113998413086\n",
            " Epoch[75|200] batch 600 Dloss:0.21255837380886078, gen_loss:20.246437072753906\n",
            " Epoch[75|200] batch 800 Dloss:0.16037487983703613, gen_loss:20.46763801574707\n",
            " Epoch[75|200] batch 1000 Dloss:0.23851457238197327, gen_loss:19.376935958862305\n",
            " Epoch[76|200] batch 200 Dloss:0.15570610761642456, gen_loss:19.948917388916016\n",
            " Epoch[76|200] batch 400 Dloss:0.2355147898197174, gen_loss:20.372817993164062\n",
            " Epoch[76|200] batch 600 Dloss:0.21573211252689362, gen_loss:20.110925674438477\n",
            " Epoch[76|200] batch 800 Dloss:0.13093942403793335, gen_loss:19.748455047607422\n",
            " Epoch[76|200] batch 1000 Dloss:1.6286448240280151, gen_loss:21.63201904296875\n",
            " Epoch[77|200] batch 200 Dloss:0.1580655723810196, gen_loss:20.766332626342773\n",
            " Epoch[77|200] batch 400 Dloss:0.31016215682029724, gen_loss:19.18505859375\n",
            " Epoch[77|200] batch 600 Dloss:0.25130021572113037, gen_loss:19.690956115722656\n",
            " Epoch[77|200] batch 800 Dloss:0.106821209192276, gen_loss:19.847436904907227\n",
            " Epoch[77|200] batch 1000 Dloss:0.5619125366210938, gen_loss:18.497468948364258\n",
            " Epoch[78|200] batch 200 Dloss:0.11395342648029327, gen_loss:20.515352249145508\n",
            " Epoch[78|200] batch 400 Dloss:0.3154209852218628, gen_loss:21.083452224731445\n",
            " Epoch[78|200] batch 600 Dloss:0.16795837879180908, gen_loss:21.02622413635254\n",
            " Epoch[78|200] batch 800 Dloss:0.10400242358446121, gen_loss:19.777816772460938\n",
            " Epoch[78|200] batch 1000 Dloss:0.30117812752723694, gen_loss:18.97113800048828\n",
            " Epoch[79|200] batch 200 Dloss:0.13967037200927734, gen_loss:20.49506950378418\n",
            " Epoch[79|200] batch 400 Dloss:0.3228137791156769, gen_loss:19.400283813476562\n",
            " Epoch[79|200] batch 600 Dloss:0.3703320622444153, gen_loss:19.45953941345215\n",
            " Epoch[79|200] batch 800 Dloss:0.1827545166015625, gen_loss:19.98382568359375\n",
            " Epoch[79|200] batch 1000 Dloss:0.561779797077179, gen_loss:19.092418670654297\n",
            " Epoch[80|200] batch 200 Dloss:0.2641676664352417, gen_loss:21.09389877319336\n",
            " Epoch[80|200] batch 400 Dloss:0.2224639356136322, gen_loss:20.540912628173828\n",
            " Epoch[80|200] batch 600 Dloss:0.40677154064178467, gen_loss:18.657306671142578\n",
            " Epoch[80|200] batch 800 Dloss:0.19294965267181396, gen_loss:20.242420196533203\n",
            " Epoch[80|200] batch 1000 Dloss:0.43412765860557556, gen_loss:19.89160919189453\n",
            " Epoch[81|200] batch 200 Dloss:0.10477852821350098, gen_loss:20.334461212158203\n",
            " Epoch[81|200] batch 400 Dloss:0.16595780849456787, gen_loss:20.00856590270996\n",
            " Epoch[81|200] batch 600 Dloss:0.3702135682106018, gen_loss:18.78594970703125\n",
            " Epoch[81|200] batch 800 Dloss:0.24133731424808502, gen_loss:20.81500244140625\n",
            " Epoch[81|200] batch 1000 Dloss:0.5750346779823303, gen_loss:18.877643585205078\n",
            " Epoch[82|200] batch 200 Dloss:0.13398033380508423, gen_loss:20.234970092773438\n",
            " Epoch[82|200] batch 400 Dloss:0.253521203994751, gen_loss:20.443185806274414\n",
            " Epoch[82|200] batch 600 Dloss:0.16541995108127594, gen_loss:20.505149841308594\n",
            " Epoch[82|200] batch 800 Dloss:0.16801530122756958, gen_loss:20.321819305419922\n",
            " Epoch[82|200] batch 1000 Dloss:0.14185217022895813, gen_loss:20.003231048583984\n",
            " Epoch[83|200] batch 200 Dloss:0.1303175687789917, gen_loss:20.62350845336914\n",
            " Epoch[83|200] batch 400 Dloss:0.18299560248851776, gen_loss:20.376800537109375\n",
            " Epoch[83|200] batch 600 Dloss:0.32810431718826294, gen_loss:19.526784896850586\n",
            " Epoch[83|200] batch 800 Dloss:0.18148572742938995, gen_loss:20.638944625854492\n",
            " Epoch[83|200] batch 1000 Dloss:0.41431063413619995, gen_loss:19.392677307128906\n",
            " Epoch[84|200] batch 200 Dloss:0.16693171858787537, gen_loss:20.57976722717285\n",
            " Epoch[84|200] batch 400 Dloss:0.19957786798477173, gen_loss:19.75779151916504\n",
            " Epoch[84|200] batch 600 Dloss:0.14423637092113495, gen_loss:20.80194664001465\n",
            " Epoch[84|200] batch 800 Dloss:0.16118445992469788, gen_loss:20.150522232055664\n",
            " Epoch[84|200] batch 1000 Dloss:1.610863208770752, gen_loss:19.64225959777832\n",
            " Epoch[85|200] batch 200 Dloss:0.18790070712566376, gen_loss:19.803142547607422\n",
            " Epoch[85|200] batch 400 Dloss:0.15291553735733032, gen_loss:20.342376708984375\n",
            " Epoch[85|200] batch 600 Dloss:0.2744395136833191, gen_loss:19.508975982666016\n",
            " Epoch[85|200] batch 800 Dloss:0.13575510680675507, gen_loss:20.53436279296875\n",
            " Epoch[85|200] batch 1000 Dloss:0.5169158577919006, gen_loss:18.553361892700195\n",
            " Epoch[86|200] batch 200 Dloss:0.07202795147895813, gen_loss:20.43168067932129\n",
            " Epoch[86|200] batch 400 Dloss:0.18982401490211487, gen_loss:20.130970001220703\n",
            " Epoch[86|200] batch 600 Dloss:0.16494563221931458, gen_loss:20.423669815063477\n",
            " Epoch[86|200] batch 800 Dloss:0.14770111441612244, gen_loss:20.003286361694336\n",
            " Epoch[86|200] batch 1000 Dloss:0.11543652415275574, gen_loss:20.323135375976562\n",
            " Epoch[87|200] batch 200 Dloss:0.2169084995985031, gen_loss:19.99241065979004\n",
            " Epoch[87|200] batch 400 Dloss:0.15498587489128113, gen_loss:20.511962890625\n",
            " Epoch[87|200] batch 600 Dloss:0.20641633868217468, gen_loss:20.187999725341797\n",
            " Epoch[87|200] batch 800 Dloss:0.11180604994297028, gen_loss:20.33052635192871\n",
            " Epoch[87|200] batch 1000 Dloss:0.34029218554496765, gen_loss:19.70359992980957\n",
            " Epoch[88|200] batch 200 Dloss:0.10245516896247864, gen_loss:20.436153411865234\n",
            " Epoch[88|200] batch 400 Dloss:0.2145843803882599, gen_loss:19.457904815673828\n",
            " Epoch[88|200] batch 600 Dloss:0.21016865968704224, gen_loss:19.821725845336914\n",
            " Epoch[88|200] batch 800 Dloss:0.14498823881149292, gen_loss:20.351043701171875\n",
            " Epoch[88|200] batch 1000 Dloss:0.3416726887226105, gen_loss:19.292543411254883\n",
            " Epoch[89|200] batch 200 Dloss:0.10353897511959076, gen_loss:21.037799835205078\n",
            " Epoch[89|200] batch 400 Dloss:0.17783401906490326, gen_loss:20.06414794921875\n",
            " Epoch[89|200] batch 600 Dloss:0.39479342103004456, gen_loss:19.832069396972656\n",
            " Epoch[89|200] batch 800 Dloss:0.15574710071086884, gen_loss:20.850557327270508\n",
            " Epoch[89|200] batch 1000 Dloss:0.3813895285129547, gen_loss:19.12820816040039\n",
            " Epoch[90|200] batch 200 Dloss:0.09962296485900879, gen_loss:20.120548248291016\n",
            " Epoch[90|200] batch 400 Dloss:0.17309491336345673, gen_loss:20.394193649291992\n",
            " Epoch[90|200] batch 600 Dloss:0.15018655359745026, gen_loss:20.388830184936523\n",
            " Epoch[90|200] batch 800 Dloss:3.818786144256592, gen_loss:20.514650344848633\n",
            " Epoch[90|200] batch 1000 Dloss:0.15773853659629822, gen_loss:19.92163848876953\n",
            " Epoch[91|200] batch 200 Dloss:0.08514084666967392, gen_loss:20.52345848083496\n",
            " Epoch[91|200] batch 400 Dloss:0.17722070217132568, gen_loss:20.676860809326172\n",
            " Epoch[91|200] batch 600 Dloss:0.364213764667511, gen_loss:19.997385025024414\n",
            " Epoch[91|200] batch 800 Dloss:0.26081037521362305, gen_loss:19.667186737060547\n",
            " Epoch[91|200] batch 1000 Dloss:0.35983529686927795, gen_loss:19.423540115356445\n",
            " Epoch[92|200] batch 200 Dloss:0.14366814494132996, gen_loss:20.34575080871582\n",
            " Epoch[92|200] batch 400 Dloss:0.2559451162815094, gen_loss:21.086143493652344\n",
            " Epoch[92|200] batch 600 Dloss:0.14569495618343353, gen_loss:20.640647888183594\n",
            " Epoch[92|200] batch 800 Dloss:0.09328430891036987, gen_loss:20.193321228027344\n",
            " Epoch[92|200] batch 1000 Dloss:0.176059752702713, gen_loss:20.057327270507812\n",
            " Epoch[93|200] batch 200 Dloss:0.174189031124115, gen_loss:21.32508087158203\n",
            " Epoch[93|200] batch 400 Dloss:0.23658716678619385, gen_loss:20.131877899169922\n",
            " Epoch[93|200] batch 600 Dloss:0.11735271662473679, gen_loss:20.545940399169922\n",
            " Epoch[93|200] batch 800 Dloss:0.1344296783208847, gen_loss:20.604578018188477\n",
            " Epoch[93|200] batch 1000 Dloss:0.5623579025268555, gen_loss:19.03493881225586\n",
            " Epoch[94|200] batch 200 Dloss:5.5014119148254395, gen_loss:21.063217163085938\n",
            " Epoch[94|200] batch 400 Dloss:0.1636970341205597, gen_loss:19.86740493774414\n",
            " Epoch[94|200] batch 600 Dloss:0.3127446174621582, gen_loss:19.86198616027832\n",
            " Epoch[94|200] batch 800 Dloss:0.19656798243522644, gen_loss:20.640241622924805\n",
            " Epoch[94|200] batch 1000 Dloss:0.4432483911514282, gen_loss:19.7772216796875\n",
            " Epoch[95|200] batch 200 Dloss:0.11575885117053986, gen_loss:20.601472854614258\n",
            " Epoch[95|200] batch 400 Dloss:0.17569610476493835, gen_loss:20.439411163330078\n",
            " Epoch[95|200] batch 600 Dloss:0.11008559167385101, gen_loss:20.505878448486328\n",
            " Epoch[95|200] batch 800 Dloss:0.07372060418128967, gen_loss:20.144222259521484\n",
            " Epoch[95|200] batch 1000 Dloss:0.5428389310836792, gen_loss:19.3044490814209\n",
            " Epoch[96|200] batch 200 Dloss:0.11335944384336472, gen_loss:20.57769203186035\n",
            " Epoch[96|200] batch 400 Dloss:0.1865626871585846, gen_loss:20.22691535949707\n",
            " Epoch[96|200] batch 600 Dloss:0.14388170838356018, gen_loss:20.884929656982422\n",
            " Epoch[96|200] batch 800 Dloss:0.19637106359004974, gen_loss:19.84941864013672\n",
            " Epoch[96|200] batch 1000 Dloss:0.5386287569999695, gen_loss:20.109233856201172\n",
            " Epoch[97|200] batch 200 Dloss:0.10404984652996063, gen_loss:20.3203182220459\n",
            " Epoch[97|200] batch 400 Dloss:0.18448685109615326, gen_loss:20.150375366210938\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "batches = len(train_dl)\n",
        "print('total batches',batches)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs+1): \n",
        "    \n",
        "    counter=0\n",
        "    for (input_img, target_img), _ in train_dl:\n",
        "        counter+=1\n",
        "        input_img = input_img.to(device)\n",
        "        target_img = target_img.to(device)\n",
        "\n",
        "        # generator forward pass\n",
        "        generated_image = generator(input_img)\n",
        "        \n",
        "        # train discriminator \n",
        "        real_target,D_total_loss,D_real_loss,D_fake_loss = train_discrminator(input_img,generated_image,D_optimizer)\n",
        "        \n",
        "        # Train generator with real labels        \n",
        "        G_loss = train_generator(input_img,generated_image,real_target,G_optimizer)\n",
        "\n",
        "        # log to tensorboard\n",
        "        iters = counter+epoch * batches + 1 \n",
        "        writer.add_scalar(\"Train_Adversarial/D_Loss\", D_total_loss, iters)\n",
        "        writer.add_scalar(\"Train_Adversarial/G_Loss\", G_loss, iters)\n",
        "        writer.add_scalar(\"Train_Adversarial/D_Real_loss\", D_real_loss, iters)\n",
        "        writer.add_scalar(\"Train_Adversarial/D_Fake_loss\", D_fake_loss, iters)\n",
        "\n",
        "        # print loss after every 200 batches\n",
        "        if(counter%200==0):\n",
        "            print(f' Epoch[{epoch}|{epochs}] batch {counter} Dloss:{D_total_loss}, gen_loss:{G_loss}')\n",
        "    \n",
        "    # after every epoch\n",
        "    # Save generated imagess\n",
        "    save_samples(epoch) # this also computes the FID and IS scores\n",
        "\n",
        "     # save model after every 5 epochs\n",
        "    if(epoch%5==0): \n",
        "        torch.save(discriminator.state_dict(), os.path.join(models_dir, f\"discr_{epoch}.pth\"))\n",
        "        torch.save(generator.state_dict(), os.path.join(models_dir, f\"genr_{epoch}.pth\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "aerial_to_street.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}